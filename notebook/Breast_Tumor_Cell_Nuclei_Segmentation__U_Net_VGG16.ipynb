{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1 - Business Problem**"
      ],
      "metadata": {
        "id": "fJARhvhzev1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Business Problem: Automated Nuclei Segmentation for Breast Cancer Diagnosis Support**\n",
        "\n",
        "\n",
        "**Business Context**\n",
        "\n",
        "Breast cancer remains one of the leading causes of cancer-related deaths worldwide, especially in aggressive subtypes like **Triple Negative Breast Cancer (TNBC)**. Accurate and timely analysis of histopathology slides is critical for effective diagnosis and treatment planning. However, manual annotation and segmentation of cell nuclei in Hematoxylin and Eosin (H\\&E) stained slides is time-consuming, subjective, and prone to inter-pathologist variability.\n",
        "\n",
        "**Business Challenge**\n",
        "\n",
        "Hospitals, diagnostic labs, and cancer research centers face the following challenges:\n",
        "\n",
        "* **Limited Pathologist Resources:** The global shortage of trained pathologists leads to diagnostic delays.\n",
        "* **Annotation Bottleneck:** Manual segmentation of nuclei is labor-intensive and not scalable for large patient volumes.\n",
        "* **Inter-observer Variability:** Manual interpretations vary significantly between pathologists, affecting diagnostic consistency.\n",
        "* **Need for Quantitative Analysis:** Automated and consistent nuclear segmentation is essential for downstream tasks such as cancer grading, prognosis estimation, and treatment decision support.\n",
        "\n",
        "**Business Objective**\n",
        "\n",
        "The objective of this project is to develop a **Deep Learning-based automated nuclei segmentation system** for breast tumor histopathology images, with special focus on TNBC samples.\n",
        "\n",
        "The solution aims to:\n",
        "\n",
        "* Automatically detect and segment cell nuclei in H\\&E-stained breast tumor images.\n",
        "* Improve speed and accuracy compared to manual annotations.\n",
        "* Support pathologists in making faster and more consistent diagnostic decisions.\n",
        "* Enable large-scale analysis of patient samples for clinical and research applications.\n",
        "\n",
        "**Expected Business Impact**\n",
        "\n",
        "**Increased diagnostic efficiency**\n",
        "**Reduced workload for pathologists**\n",
        "**Improved accuracy and consistency in cancer diagnosis**\n",
        "**Better patient outcomes through timely treatment decisions**\n",
        "\n",
        "This project will serve as a critical step towards implementing AI-powered digital pathology solutions in real-world clinical settings."
      ],
      "metadata": {
        "id": "ZLN4ee77e9sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 - API Kaggle**"
      ],
      "metadata": {
        "id": "kYDUzIxse0sA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YF-x97QVYop"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "id": "venrQUoXe2Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "_sSOm-J2e2Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "r29kZ8-7e2WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "2xi2dUCCe2Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3 - Baixando dataset**"
      ],
      "metadata": {
        "id": "15YAJCTve6gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading database from kaggle\n",
        "!kaggle datasets download tuanledinh/processedoriginalmonuseg"
      ],
      "metadata": {
        "id": "lmc-IfvQe6vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target directory where the dataset will be extracted\n",
        "target_dir = \"/content/dataset/CNN/Breast\""
      ],
      "metadata": {
        "id": "BV2kAPIpfd3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the zip file containing the Bitcoin tweets dataset\n",
        "zf = \"/content/processedoriginalmonuseg.zip\""
      ],
      "metadata": {
        "id": "rv-Ere8efza0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the zip file containing the dataset\n",
        "zfile = zipfile.ZipFile(zf)"
      ],
      "metadata": {
        "id": "97PtF6ehfd9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all contents of the zip file to the target directory\n",
        "zfile.extractall(target_dir)"
      ],
      "metadata": {
        "id": "10Gq-JjRfeBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4 - Importando as bibliotecas**"
      ],
      "metadata": {
        "id": "EnRVgUvWgMlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "\n",
        "#\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "#\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#\n",
        "from PIL import Image\n",
        "\n",
        "#\n",
        "from glob import glob\n",
        "\n",
        "#\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Dropout\n",
        "\n",
        "#\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "#\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "#\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Vbmj6EkYgLPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 5 - Database**"
      ],
      "metadata": {
        "id": "XK0ux56ThOV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_and_masks(img_dir, mask_dir, img_size=(256, 256)):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    img_files = sorted(os.listdir(img_dir))\n",
        "\n",
        "    for file_name in img_files:\n",
        "        img_path = os.path.join(img_dir, file_name)\n",
        "        mask_path = os.path.join(mask_dir, file_name)\n",
        "\n",
        "        # Carrega imagem\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, img_size)\n",
        "        img = img / 255.0  # Normaliza para 0-1\n",
        "\n",
        "        # Carrega máscara\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, img_size)\n",
        "        mask = mask / 255.0  # Normaliza máscara binária (0-1)\n",
        "        mask = np.expand_dims(mask, axis=-1)  # Formato (H, W, 1)\n",
        "\n",
        "        images.append(img)\n",
        "        masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)"
      ],
      "metadata": {
        "id": "XGAb3wa1hKA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminhos das pastas\n",
        "\n",
        "#\n",
        "train_img_dir = \"/content/dataset/CNN/Breast/processed-original-monuseg/train_folder/img\"\n",
        "\n",
        "#\n",
        "train_mask_dir = \"/content/dataset/CNN/Breast/processed-original-monuseg/train_folder/labelcol\""
      ],
      "metadata": {
        "id": "_6DEOHi3hUKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_samples(X, Y, num_samples=6):\n",
        "    plt.figure(figsize=(12, num_samples * 2))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Imagem original\n",
        "        plt.subplot(num_samples, 2, 2*i+1)\n",
        "        plt.imshow(X[i])\n",
        "        plt.title(f'Input Image {i+1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Máscara correspondente\n",
        "        plt.subplot(num_samples, 2, 2*i+2)\n",
        "        plt.imshow(Y[i].squeeze(), cmap='gray')\n",
        "        plt.title(f'Mask {i+1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U4Uhg95JiEFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar os dados\n",
        "X_train, Y_train = load_images_and_masks(train_img_dir, train_mask_dir)"
      ],
      "metadata": {
        "id": "4d9p8FEGiEMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar\n",
        "visualize_samples(X_train, Y_train, num_samples=6)"
      ],
      "metadata": {
        "id": "VbzeKJ7Av-s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rede Neural - UNET**"
      ],
      "metadata": {
        "id": "50NS_5bCipkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    # Bottleneck\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    # Decoder\n",
        "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = concatenate([u9, c1])\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "#\n",
        "def dice_coef(y_true, y_pred, smooth=1e-7):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "#\n",
        "def iou_metric(y_true, y_pred, smooth=1e-7):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "model = unet_model()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', iou_metric, dice_coef])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', iou_metric, dice_coef])\n",
        "\n",
        "#\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "o09kXHSciCQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinamento rede neural\n",
        "history_unet = model.fit(X_train, Y_train,\n",
        "                         epochs=120,\n",
        "                         batch_size=8,\n",
        "                         validation_split=0.2)"
      ],
      "metadata": {
        "id": "6WX9TfL8iVk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_unet.history['loss'], label='Training Loss')\n",
        "    plt.plot(history_unet.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_unet.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history_unet.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualizando desempenho da rede\n",
        "plot_training_history(history_unet)"
      ],
      "metadata": {
        "id": "H9H_IU4vkxAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def plot_iou_and_dice_training_seaborn(history):\n",
        "    sns.set(style=\"whitegrid\")  # Estilo de fundo\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # === IoU ===\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.lineplot(x=range(len(history_unet.history['iou_metric'])), y=history_unet.history['iou_metric'], label='Training IoU', color='blue')\n",
        "    sns.lineplot(x=range(len(history_unet.history['val_iou_metric'])), y=history_unet.history['val_iou_metric'], label='Validation IoU', color='orange')\n",
        "    plt.title('IoU Metric During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('IoU')\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # === Dice ===\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.lineplot(x=range(len(history_unet.history['dice_coef'])), y=history_unet.history['dice_coef'], label='Training Dice', color='green')\n",
        "    sns.lineplot(x=range(len(history_unet.history['val_dice_coef'])), y=history_unet.history['val_dice_coef'], label='Validation Dice', color='red')\n",
        "    plt.title('Dice Coefficient During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Dice Coefficient')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# Exemplo de uso:\n",
        "plot_iou_and_dice_training_seaborn(history_unet)"
      ],
      "metadata": {
        "id": "muGDD9Zn7THL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando uma imagem de exemplo\n",
        "i = 0\n",
        "test_img = X_train[i]\n",
        "ground_truth = Y_train[i]"
      ],
      "metadata": {
        "id": "SLOeTKCniXrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "pred = model.predict(np.expand_dims(test_img, axis=0))[0]"
      ],
      "metadata": {
        "id": "IfUlwhgKianN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_segmentation_overlay(test_img, ground_truth, pred_mask):\n",
        "    plt.figure(figsize=(15,5))\n",
        "\n",
        "    # Imagem original\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.title('Input Image')\n",
        "    plt.imshow(test_img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Ground Truth\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.title('Ground Truth Mask')\n",
        "    plt.imshow(ground_truth.squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Predicted Mask\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.title('Predicted Mask')\n",
        "    plt.imshow(pred_mask.squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Overlay com Colormap (ex: Jet)\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.title('Overlay Prediction')\n",
        "    overlay = plt.imshow(test_img, alpha=0.7)\n",
        "    plt.imshow(pred_mask.squeeze(), cmap='jet', alpha=0.3)  # Ajuste alpha como quiser\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#\n",
        "plot_segmentation_overlay(test_img, ground_truth, pred)"
      ],
      "metadata": {
        "id": "wLisQMhZibl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "pred_binary = (pred > 0.5).astype(np.uint8)\n",
        "plot_segmentation_overlay(test_img, ground_truth, pred_binary)"
      ],
      "metadata": {
        "id": "ZV-c_JfD9eE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_and_save_predictions(X, Y_true, Y_pred, num_samples=6, save_path=\"segmentation_results.png\", threshold=0.5, cmap_overlay='plasma'):\n",
        "    plt.figure(figsize=(16, num_samples * 3))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Binariza a predição\n",
        "        pred_thresh = (Y_pred[i] > threshold).astype(np.float32)\n",
        "\n",
        "        # Calcula métricas\n",
        "        iou = calculate_iou(Y_true[i], pred_thresh)\n",
        "        dice = calculate_dice(Y_true[i], pred_thresh)\n",
        "\n",
        "        # Input Image\n",
        "        plt.subplot(num_samples, 4, 4*i + 1)\n",
        "        plt.imshow(X[i])\n",
        "        plt.title(f\"Input {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Ground Truth Mask\n",
        "        plt.subplot(num_samples, 4, 4*i + 2)\n",
        "        plt.imshow(Y_true[i].squeeze(), cmap='gray')\n",
        "        plt.title(f\"Ground Truth {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Predicted Mask (Thresholded)\n",
        "        plt.subplot(num_samples, 4, 4*i + 3)\n",
        "        im_pred = plt.imshow(pred_thresh.squeeze(), cmap='gray')\n",
        "        plt.title(f\"Predicted {i+1}\\nIoU: {iou:.2f} | Dice: {dice:.2f}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Overlay (Input + Probabilidade com Colorbar)\n",
        "        plt.subplot(num_samples, 4, 4*i + 4)\n",
        "        plt.imshow(X[i])\n",
        "        im_overlay = plt.imshow(Y_pred[i].squeeze(), cmap=cmap_overlay, alpha=0.5)\n",
        "        plt.title(f\"Overlay {i+1}\")\n",
        "\n",
        "        # Colorbar apenas para overlay\n",
        "        cbar = plt.colorbar(im_overlay, fraction=0.046, pad=0.04)\n",
        "        cbar.set_label('Predicted Probability', rotation=270, labelpad=15)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "    print(f\"\\n Resultado salvo em: {save_path}\")\n",
        "\n",
        "#\n",
        "def calculate_iou(y_true, y_pred, smooth=1e-7):\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    union = np.sum(y_true_f) + np.sum(y_pred_f) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "#\n",
        "def calculate_dice(y_true, y_pred, smooth=1e-7):\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)"
      ],
      "metadata": {
        "id": "ccLCCr6gkpQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsão modelo\n",
        "num_samples = 6\n",
        "X_test = X_train[:num_samples]\n",
        "Y_test = Y_train[:num_samples]\n",
        "Y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "ztQ0W6X0mBsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsão\n",
        "plot_and_save_predictions(X_test, Y_test, Y_pred, num_samples=num_samples, save_path=\"nuclei_segmentation_grid.png\", threshold=0.5, cmap_overlay='plasma')\n",
        "\n",
        "# Salvando imagem previsão\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"breast_nuclei_segmentation_results.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WJ543ZjPk9Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvando rede\n",
        "model.save(\"unet_UNet_breast_segmentation.h5\")"
      ],
      "metadata": {
        "id": "5iVckp-v356x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metricas e avaliação**"
      ],
      "metadata": {
        "id": "J3N7rNx-oKUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo usando um batch de teste\n",
        "num_samples = 10\n",
        "X_test = X_train[:num_samples]\n",
        "Y_test = Y_train[:num_samples]\n",
        "\n",
        "# Faz a predição\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# Binariza a predição\n",
        "Y_pred_binary = (Y_pred > 0.3).astype(np.uint8)\n",
        "\n",
        "# Binariza também o Ground Truth (se tiver ruídos ou for float)\n",
        "Y_test_binary = (Y_test > 0.3).astype(np.uint8)\n",
        "\n",
        "# Flatten ambos para 1D\n",
        "y_true_flat = Y_test_binary.flatten()\n",
        "y_pred_flat = Y_pred_binary.flatten()\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(y_true_flat, y_pred_flat, target_names=[\"Background\", \"Nuclei\"])\n",
        "print(report)"
      ],
      "metadata": {
        "id": "TnIvSVeonGRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a predição nas amostras de teste\n",
        "num_samples = 10\n",
        "X_test = X_train[:num_samples]\n",
        "Y_test = Y_train[:num_samples]\n",
        "\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# Ground Truth binário\n",
        "Y_test_binary = (Y_test > 0.3).astype(np.uint8)\n",
        "\n",
        "# Flatten para 1D\n",
        "y_true_flat = Y_test_binary.flatten()\n",
        "y_pred_flat = Y_pred.flatten()  # Aqui usamos a probabilidade contínua (antes do threshold)\n",
        "\n",
        "# Calcular a curva ROC e AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_true_flat, y_pred_flat)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotar a Curva ROC\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Pixel-wise Segmentation')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8lVRgQiUnbQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo\n",
        "precision, recall, thresholds_pr = precision_recall_curve(y_true_flat, y_pred_flat)\n",
        "average_precision = average_precision_score(y_true_flat, y_pred_flat)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(recall, precision, color='purple', lw=2, label=f'AP = {average_precision:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Pixel-wise Segmentation')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a2xq-m8Ontau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo 2 - Rede Neural**"
      ],
      "metadata": {
        "id": "gDhZISdIimMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0, MobileNetV2\n",
        "import numpy as np\n",
        "\n",
        "# ===== Métricas personalizadas =====\n",
        "def iou_metric(y_true, y_pred, smooth=1e-7):\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    union = np.sum(y_true_f) + np.sum(y_pred_f) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1e-7):\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
        "\n",
        "# ===== U-Net Básico =====\n",
        "def unet_model(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = concatenate([u9, c1])\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "# ===== Função para U-Net com backbone =====\n",
        "def build_unet_with_backbone(base_model, skip_layers, input_size=(256, 256, 3)):\n",
        "    inputs = base_model.input\n",
        "    x = base_model.output\n",
        "    skips = [base_model.get_layer(name).output for name in skip_layers][::-1]\n",
        "\n",
        "    for skip in skips:\n",
        "        while x.shape[1] < skip.shape[1] or x.shape[2] < skip.shape[2]:\n",
        "            x = Conv2DTranspose(skip.shape[-1], (2, 2), strides=(2, 2), padding='same')(x)\n",
        "        x = concatenate([x, skip])\n",
        "        x = Conv2D(skip.shape[-1], (3, 3), activation='relu', padding='same')(x)\n",
        "        x = Conv2D(skip.shape[-1], (3, 3), activation='relu', padding='same')(x)\n",
        "\n",
        "    while x.shape[1] < input_size[0] or x.shape[2] < input_size[1]:\n",
        "        x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(x)\n",
        "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# ===== Modelos =====\n",
        "def unet_baseline(input_size=(256, 256, 3)):\n",
        "    return unet_model(input_size)\n",
        "\n",
        "def unet_vgg16(input_size=(256, 256, 3)):\n",
        "    base = VGG16(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "    skip = [\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\"]\n",
        "    return build_unet_with_backbone(base, skip, input_size)\n",
        "\n",
        "def unet_resnet50(input_size=(256, 256, 3)):\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "    skip = [\"conv1_relu\", \"conv2_block3_out\", \"conv3_block4_out\", \"conv4_block6_out\"]\n",
        "    return build_unet_with_backbone(base, skip, input_size)\n",
        "\n",
        "def unet_efficientnetb0(input_size=(256, 256, 3)):\n",
        "    base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "    skip = [\"block2a_activation\", \"block3a_activation\", \"block4a_activation\", \"block6a_activation\"]\n",
        "    return build_unet_with_backbone(base, skip, input_size)\n",
        "\n",
        "def unet_mobilenetv2(input_size=(256, 256, 3)):\n",
        "    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_size)\n",
        "    skip = [\"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\", \"block_13_expand_relu\"]\n",
        "    return build_unet_with_backbone(base, skip, input_size)\n",
        "\n",
        "# ===== Lista de modelos (agora depois das definições) =====\n",
        "model_functions = [(\"U-Net\", unet_baseline),(\"VGG16\", unet_vgg16),(\"ResNet50\", unet_resnet50),(\"EfficientNetB0\", unet_efficientnetb0),(\"MobileNetV2\", unet_mobilenetv2)]\n",
        "\n",
        "# ===== Loop para mostrar o summary de cada modelo =====\n",
        "for model_name, model_fn in model_functions:\n",
        "    print(f\"\\n============================\")\n",
        "    print(f\"MODEL SUMMARY: {model_name}\")\n",
        "    print(f\"============================\\n\")\n",
        "    model = model_fn()\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "hZ6FRN-7inqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ===== Dicionário para guardar os histories =====\n",
        "history_dict = {}\n",
        "\n",
        "# ===== Treinamento =====\n",
        "for model_name, model_fn in model_functions:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"INICIANDO TREINAMENTO: {model_name}\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    # Cria o modelo\n",
        "    model = model_fn()\n",
        "\n",
        "    # Congelando backbone\n",
        "    print()\n",
        "    print(f\"Freezing backbone para o modelo {model_name}...\")\n",
        "    print()\n",
        "    for layer in model.layers[:20]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Compilar e treinar com backbone congelado\n",
        "    print()\n",
        "    print(f\"Treinando {model_name} com backbone congelado (fase 1)...\")\n",
        "    print()\n",
        "\n",
        "    #\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    #\n",
        "    history_frozen = model.fit(X_train, Y_train, epochs=20, batch_size=8, validation_split=0.2, verbose=1)\n",
        "\n",
        "    # Liberar todas as camadas para fine-tuning\n",
        "    print()\n",
        "    print(f\"Fine-tuning (liberando todas as camadas) para o modelo {model_name}...\")\n",
        "    print()\n",
        "\n",
        "    #\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Compilar e treinar com learning rate menor\n",
        "    print()\n",
        "    print(f\"Treinando {model_name} (fase fine-tuning)...\")\n",
        "    print()\n",
        "\n",
        "    #\n",
        "    model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    #\n",
        "    history_finetune = model.fit(X_train, Y_train, epochs=30, batch_size=8, validation_split=0.2, verbose=1)\n",
        "\n",
        "    # Unir histories (fase1 + fine-tuning)\n",
        "    full_history = history_frozen\n",
        "    for key in history_finetune.history.keys():\n",
        "        full_history.history[key] = history_frozen.history.get(key, []) + history_finetune.history[key]\n",
        "\n",
        "    # Salva no dicionário\n",
        "    history_dict[model_name] = full_history\n",
        "\n",
        "    print(f\"\\nFinalizado o treinamento de: {model_name}\\n\")\n",
        "\n",
        "print(\"\\nTreinamento completo para todos os modelos!\")\n"
      ],
      "metadata": {
        "id": "grdZUs43jQ24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import os\n",
        "\n",
        "#\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "os.makedirs('weights', exist_ok=True)\n",
        "\n",
        "#\n",
        "for model_name, model_fn in model_functions:\n",
        "    print(f\"Salvando o modelo: {model_name}\")\n",
        "\n",
        "    # Recomendo recriar o modelo antes de carregar os pesos finais (se quiser, mas não obrigatório)\n",
        "    model = model_fn()\n",
        "\n",
        "    # Treinar novamente se necessário ou carregar os pesos mais recentes\n",
        "    # Aqui assumindo que você já treinou e tem a variável `model` pronta ao final de cada loop.\n",
        "\n",
        "    # Caminho de saída\n",
        "    save_path = f\"saved_models/{model_name}_breast_segmentation.keras\"\n",
        "\n",
        "    # Salvar o modelo inteiro (estrutura + pesos + optimizer)\n",
        "    print()\n",
        "    model.save(save_path)\n",
        "    print(f\"Modelo salvo em: {save_path}\")"
      ],
      "metadata": {
        "id": "MZEtJgsR_dEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isso vai listar os arquivos na pasta atual, incluindo os .h5\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "id": "Na1v6qHkNMzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_performance(history_dict):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # ======= Loss =======\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for model_name, history in history_dict.items():\n",
        "        plt.plot(history.history['loss'], label=f'{model_name} - Train Loss')\n",
        "        plt.plot(history.history['val_loss'], linestyle='--', label=f'{model_name} - Val Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(False)\n",
        "\n",
        "    # ======= Accuracy =======\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for model_name, history in history_dict.items():\n",
        "        plt.plot(history.history['accuracy'], label=f'{model_name} - Train Acc')\n",
        "        plt.plot(history.history['val_accuracy'], linestyle='--', label=f'{model_name} - Val Acc')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(False)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Plot\n",
        "plot_model_performance(history_dict)"
      ],
      "metadata": {
        "id": "_wWzpL_XqZ2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou_dice(y_true, y_pred_bin):\n",
        "    intersection = np.sum(y_true * y_pred_bin)\n",
        "    union = np.sum(y_true) + np.sum(y_pred_bin) - intersection\n",
        "    iou = intersection / (union + 1e-7)\n",
        "    dice = (2 * intersection) / (np.sum(y_true) + np.sum(y_pred_bin) + 1e-7)\n",
        "    return iou, dice\n",
        "\n",
        "def plot_model_predictions(model_name, model, X_test, Y_test, threshold=0.5, num_samples=6):\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    plt.figure(figsize=(16, num_samples * 3))\n",
        "    for i in range(num_samples):\n",
        "        # Binariza a predição\n",
        "        pred_bin = (Y_pred[i] > threshold).astype(np.float32)\n",
        "\n",
        "        # Calcula IoU e Dice\n",
        "        iou, dice = calculate_iou_dice(Y_test[i].flatten(), pred_bin.flatten())\n",
        "\n",
        "        # Input\n",
        "        plt.subplot(num_samples, 4, 4*i + 1)\n",
        "        plt.imshow(X_test[i])\n",
        "        plt.title(f\"Input {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Ground Truth\n",
        "        plt.subplot(num_samples, 4, 4*i + 2)\n",
        "        plt.imshow(Y_test[i].squeeze(), cmap='gray')\n",
        "        plt.title(f\"Ground Truth {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Predicted Binary Mask\n",
        "        plt.subplot(num_samples, 4, 4*i + 3)\n",
        "        plt.imshow(pred_bin.squeeze(), cmap='gray')\n",
        "        plt.title(f\"Pred {i+1} | IoU: {iou:.2f} | Dice: {dice:.2f}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Overlay (probabilidade original com colormap)\n",
        "        plt.subplot(num_samples, 4, 4*i + 4)\n",
        "        plt.imshow(X_test[i])\n",
        "        im_overlay = plt.imshow(Y_pred[i].squeeze(), cmap='plasma', alpha=0.5)\n",
        "        plt.title(f\"Overlay {i+1}\")\n",
        "        plt.colorbar(im_overlay, fraction=0.046, pad=0.04)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Predictions for {model_name}\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WgSDwi4WqtYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#\n",
        "model_base_path = '/content/saved_models'\n",
        "\n",
        "#\n",
        "for model_name, model_fn in model_functions:\n",
        "    print(f\"\\n============================\")\n",
        "    print(f\"Generating predictions for: {model_name}\")\n",
        "    print(f\"============================\\n\")\n",
        "\n",
        "    # Nome correto do arquivo .keras\n",
        "    keras_file = os.path.join(model_base_path, f\"{model_name}_breast_segmentation.keras\")\n",
        "\n",
        "    # Corrige se houver nome com hífen (caso do U-Net)\n",
        "    keras_file = keras_file.replace('U-Net', 'U-Net')  # Apenas por garantia\n",
        "\n",
        "    if os.path.exists(keras_file):\n",
        "        try:\n",
        "            model = load_model(keras_file, compile=False)\n",
        "            print(f\"✅ Modelo carregado de: {keras_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao carregar o modelo: {e}\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"❌ Arquivo de modelo não encontrado: {keras_file}\")\n",
        "        continue\n",
        "\n",
        "    # Faz as previsões e plota\n",
        "    plot_model_predictions(model_name, model, X_test, Y_test, threshold=0.5, num_samples=num_samples)"
      ],
      "metadata": {
        "id": "O9-n5_TxAQqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def calculate_iou_dice(y_true, y_pred):\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
        "    iou = intersection / (union + 1e-7)\n",
        "    dice = (2 * intersection) / (np.sum(y_true) + np.sum(y_pred) + 1e-7)\n",
        "    return iou, dice\n",
        "\n",
        "# Test set\n",
        "num_samples = 10\n",
        "X_test = X_train[:num_samples]\n",
        "Y_test = Y_train[:num_samples]\n",
        "\n",
        "# Binariza o Ground Truth\n",
        "Y_test_binary = (Y_test > 0.5).astype(np.uint8)\n",
        "y_true_flat = Y_test_binary.flatten()\n",
        "\n",
        "model_base_path = \"/content/saved_models\"\n",
        "\n",
        "for model_name, model_fn in model_functions:\n",
        "    print(f\"\\n============================\")\n",
        "    print(f\"Evaluation for: {model_name}\")\n",
        "    print(f\"============================\\n\")\n",
        "\n",
        "    keras_file = os.path.join(model_base_path, f\"{model_name}_breast_segmentation.keras\")\n",
        "\n",
        "    if os.path.exists(keras_file):\n",
        "        try:\n",
        "            # Carregando o modelo completo\n",
        "            model = load_model(keras_file, compile=False)\n",
        "            print(f\"✅ Modelo carregado de: {keras_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao carregar o modelo: {e}\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"❌ Arquivo não encontrado: {keras_file}\")\n",
        "        continue\n",
        "\n",
        "    # Predição\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred_binary = (Y_pred > 0.5).astype(np.uint8)\n",
        "    y_pred_flat = Y_pred_binary.flatten()\n",
        "\n",
        "    # Classification Report\n",
        "    report = classification_report(y_true_flat, y_pred_flat, target_names=[\"Background\", \"Nuclei\"])\n",
        "    print(report)\n",
        "\n",
        "    # IoU e Dice global\n",
        "    iou, dice = calculate_iou_dice(y_true_flat, y_pred_flat)\n",
        "    print(f\"IoU Médio: {iou:.4f}\")\n",
        "    print(f\"Dice Médio: {dice:.4f}\")"
      ],
      "metadata": {
        "id": "ddF5QX8ashE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "\n",
        "#\n",
        "num_samples = 10\n",
        "X_test = X_train[:num_samples]\n",
        "Y_test = Y_train[:num_samples]\n",
        "Y_test_binary = (Y_test > 0.5).astype(np.uint8)\n",
        "y_true_flat = Y_test_binary.flatten()\n",
        "\n",
        "\n",
        "#\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "#\n",
        "for model_name, model_fn in model_functions:\n",
        "    print(f\"\\n============================\")\n",
        "    print(f\"ROC Curve for: {model_name}\")\n",
        "    print(f\"============================\\n\")\n",
        "\n",
        "    #\n",
        "    model = model_fn()\n",
        "\n",
        "    # Corrigir nome do arquivo de pesos\n",
        "    if model_name == \"/content/unet_UNet_breast_segmentation.h5\":\n",
        "        weight_file = \"/content/unet_UNet_breast_segmentation.h5\"\n",
        "    else:\n",
        "        clean_name = model_name.replace('U-Net + ', '').replace(' ', '')\n",
        "        weight_file = f\"unet_{clean_name}_breast_segmentation.h5\"\n",
        "\n",
        "    try:\n",
        "        model.load_weights(weight_file)\n",
        "        print(f\"Pesos carregados de: {weight_file}\")\n",
        "    except:\n",
        "        print(f\"Arquivo {weight_file} não encontrado. Pulando...\")\n",
        "        continue\n",
        "\n",
        "    # Faz predição com as probabilidades (sem threshold)\n",
        "    Y_pred = model.predict(X_test)\n",
        "    y_pred_flat = Y_pred.flatten()\n",
        "\n",
        "    # Calcula curva ROC\n",
        "    fpr, tpr, thresholds = roc_curve(y_true_flat, y_pred_flat)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
        "\n",
        "# Linha diagonal (baseline)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Pixel-wise Segmentation (All Models)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "txrP23cAuPH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resultados Redes Neurais**"
      ],
      "metadata": {
        "id": "EbfOiMTE5GUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Geração de DataFrame com Resultados =====\n",
        "\n",
        "#\n",
        "results = []\n",
        "\n",
        "#\n",
        "for model_name, history in history_dict.items():\n",
        "    min_loss = min(history.history['val_loss'])\n",
        "    max_acc = max(history.history['val_accuracy'])\n",
        "\n",
        "    #\n",
        "    result = {'Model': model_name,\n",
        "              'Best Val Loss': round(min_loss, 4),\n",
        "              'Best Val Accuracy': round(max_acc, 4)}\n",
        "\n",
        "    # Se tiver IoU e Dice no history:\n",
        "    if 'val_iou_metric' in history.history and 'val_dice_coef' in history.history:\n",
        "        result['Best Val IoU'] = round(max(history.history['val_iou_metric']), 4)\n",
        "        result['Best Val Dice'] = round(max(history.history['val_dice_coef']), 4)\n",
        "\n",
        "    #\n",
        "    results.append(result)\n",
        "\n",
        "#\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "#\n",
        "df_results = df_results.sort_values(by='Best Val Accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "#\n",
        "df_results"
      ],
      "metadata": {
        "id": "2w02IkEA5LIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou_dice(y_true, y_pred_bin):\n",
        "    intersection = np.sum(y_true * y_pred_bin)\n",
        "    union = np.sum(y_true) + np.sum(y_pred_bin) - intersection\n",
        "    iou = intersection / (union + 1e-7)\n",
        "    dice = (2 * intersection) / (np.sum(y_true) + np.sum(y_pred_bin) + 1e-7)\n",
        "    return iou, dice\n",
        "\n",
        "# Test set\n",
        "num_samples = 10\n",
        "X_test = X_train[:num_samples]\n",
        "Y_test = Y_train[:num_samples]\n",
        "Y_test_binary = (Y_test > 0.5).astype(np.uint8)\n",
        "y_true_flat = Y_test_binary.flatten()\n",
        "\n",
        "# Lista de resultados\n",
        "final_results = []\n",
        "\n",
        "# Caminho onde estão os modelos .keras\n",
        "model_base_path = \"/content/saved_models\"\n",
        "\n",
        "for model_name, model_fn in model_functions:\n",
        "    print(f\"\\n========== Avaliando modelo: {model_name} ==========\\n\")\n",
        "\n",
        "    keras_file = os.path.join(model_base_path, f\"{model_name}_breast_segmentation.keras\")\n",
        "\n",
        "    if not os.path.exists(keras_file):\n",
        "        print(f\"❌ Modelo não encontrado: {keras_file}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Carregar o modelo completo\n",
        "        model = load_model(keras_file, compile=False)\n",
        "        print(f\"✅ Modelo carregado: {keras_file}\")\n",
        "\n",
        "        # Predição\n",
        "        Y_pred = model.predict(X_test)\n",
        "        Y_pred_binary = (Y_pred > 0.5).astype(np.uint8)\n",
        "        y_pred_flat = Y_pred_binary.flatten()\n",
        "\n",
        "        # Classification report\n",
        "        report_dict = classification_report(y_true_flat, y_pred_flat, target_names=[\"Background\", \"Nuclei\"], output_dict=True)\n",
        "        iou, dice = calculate_iou_dice(y_true_flat, y_pred_flat)\n",
        "\n",
        "        result = {\n",
        "            'Model': model_name,\n",
        "            'Accuracy': round(report_dict['accuracy'], 4),\n",
        "            'Precision (Nuclei)': round(report_dict['Nuclei']['precision'], 4),\n",
        "            'Recall (Nuclei)': round(report_dict['Nuclei']['recall'], 4),\n",
        "            'F1-Score (Nuclei)': round(report_dict['Nuclei']['f1-score'], 4),\n",
        "            'Support (Nuclei)': int(report_dict['Nuclei']['support']),\n",
        "            'Macro Avg F1': round(report_dict['macro avg']['f1-score'], 4),\n",
        "            'Weighted Avg F1': round(report_dict['weighted avg']['f1-score'], 4),\n",
        "            'IoU Médio': round(iou, 4),\n",
        "            'Dice Médio': round(dice, 4)\n",
        "        }\n",
        "\n",
        "        final_results.append(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao avaliar o modelo {model_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Resultado final como DataFrame\n",
        "df_metrics = pd.DataFrame(final_results)\n",
        "df_metrics = df_metrics.sort_values(by='Dice Médio', ascending=False).reset_index(drop=True)\n",
        "df_metrics"
      ],
      "metadata": {
        "id": "gHNFif7lPG_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}